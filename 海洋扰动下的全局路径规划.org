#+STARTUP: latexpreview
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
* AI-a modern approach
** 非确定动作的搜索
感知在环境是部分观测或者是非确定的情况下会非常有用：
+ 部分观测环境中：每一此感知帮助缩小代理可能存在的状态空间。
+ 非确定环境：观测告知代理其动作的可能结果。

在这两种情况下得出的规划结果被称为应急计划，其策略是依据所接受到的感知信息所决定的。
应急策略的表示更多的是以if-then-else的结构的形式来表示：
如[Suck, if State=5 then [Right,Suck] else []]

*** AND-OR搜索树
+ OR节点：代理自己的选择
+ AND节点：环境决定的结果

AND-OR搜索的结果：
+ 在每个树叶上都有一个树叶
+ 在每一个OR节点确立一个动作
+ 包含AND节点的每一个结果

*** Action fail - try try again
关键想法：不断执行知道达到完成预定动作。

** 部分观察情况下的搜索
解决这类问题的关键是：在拥有了给定动作序列和观测的情况下，
用信念空间来表示代理对于自己可能存在的状态空间。

解决的难度取决于每个信念空间的大小。

** 在线搜索和未知环境
在线搜索方法和离线搜索的不同之处在于，它的计算和动作相互重叠：
它首先采取动作，然后观察环境，最后计算下一步动作。

在线搜索方法适用于等待计算成本太高的动态和半动态的领域。
在线搜索在非确定性领域中也很有用处，
因为它允许代理专注于真正可能会发生而不是只有很小概率发生的应急方案的计算。

*** 在线计算问题
在线计算问题必须被执行动作的代理所解决，而不是纯计算。

评估-竞争率：
代理所经过的实际路径的代价和如果代理提前知道搜索空间的路径的代价的比。

DFS, backtracked.

*** 在线局部搜索
+ 登上算法
+ 结合随机行走
+ 使用存储来增进

* Planning algorithm
** Discrete planning
*** Value Iteration
根据最优方案的部分方案也为最优的观察，
设计出一个得到最优结果的迭代式算法，称为数值迭代算法。
可以求解变长规划问题，随机不确定性问题，
不完美的状态测量问题以及其他一些复杂问题。

**** Optimal Fixed-Length Plans
***** 后向数值迭代
$G^*_k(x_k)=\min_{u_k}\{l(x_k,u_k)+G^*_{k+1}(x_{k+1})\}$
$G^*_k$ 的计算的时间复杂度为 $O(|X||U|)$ 。

***** 前向数值迭代
在后向数值迭代中，$X_G$ 是固定的，而在前向数值迭代中固定的为 $x_I$ 。
$C^*_k(x_k)=\min_{u^{-1} \in U^{-1}(x)} \{C^*{k-1}(x_{k-1})+l(x_{k-1},u_{k-1})\}$

**** Optimal Plans of Unspecificed Length
该模型并不包含特定的K，而是引入了一个特殊的动作 $u_T$ 。

后向数值迭代公式：
$G^*(x)=\min_u \{l(x,u) + G^*(f(x,u))\}$

前向数值迭代公式：
$u^* = \argmin_{u^{-1} \in U^{-1}(x)} \{C^*(f^{-1}(x,u^{-1}))+l(f^{-1}(x,u^{-1}),u')\}$

** Basic Decision Theory
一个叫做自然的特殊的状态决策器被使用作为一个通用的方法来模拟不确定性。
自然在某种意义上来说是被虚构化的，因为它不是一个为了自己的利益做出智能理性决策的实体。

*** 基础概念
最优解并不总是存在。

定义一个部分排序 $<=$ ，在一系列动作上。
一个动作 $u$ 被认为是主宰另一个动作 $u$ 如果他们并不相等，
并且对于所有的 $i$ ，均有 $L_i(u)<=L_i(u')$ 。
需要注意的是很多动作是不可比较的。

一个动作被认为是Pareto optimal如果它不被任何其他的变量所主宰。

并不存在随即策略比所有确定性策略都好。

*** 和自然的博弈
**** 对自然的建模
和自然的博弈：$\theta\in\Theta$

自然知道机器人的动作： $\theta\in\Theta(u)$

**** 非确定性和概率模型
选取哪个模型取决于机器人所拥有的关于自然如何选取自己的反应动作的信息。
+ 非确定性：我完全不知道自然该如何动作。
+ 概率性：我已经观察过自然并且搜集到了有用信息。

下面来对自然的建模和自然的博弈为例来说明。

对于非确定性模型，在该种情况下的一个合理的方案是通过假设最差情况来做出决策。
但是在很多应用中，最差条件分析过于保守。
$u^*=\argmin_{u \in U}\{\max_{\theta\in\Theta}\{L(u,\Theta)\}\}$

在概率模型中，采用的是期望条件分析。
$u^*=\argmin_{u \in U}\{E_{\theta}[L(u,\theta)]\}$
$E_{\theta}[L(u,\theta)]=\sum_{\theta\in\Theta}L(u,\theta)P(\theta)$

在决策论中，关键想法是最小化后悔。
这个是当做出坏的决策后希望反悔的一种感觉。
可以使用下式描述：
$T(u,\theta)=\max_{u' \in U}\{L(u,\theta)-L(u',\theta)\}$

*** 两个玩家的零和博弈
**** 博弈方程
在本模型中，假设决策器有截然相反的利益。
在博弈中，存在两个玩家，每个都以对方的损失为代价来获得利益。
这在决策论中称为零和游戏。
$L_1(u,v)=-L_2(u,v)$

**** 确定性策略
对于P1来说，一个详细的策略u*可以定义为：
$u*=\argmin_{u \in U}\{\max_{v\inV}\{L(u,v)\}\}$

对于代价值，定义上界值和下界值如下：
$\overset{-}{L^*}=\max_{v \in V}\{L(u^*,v)\}$
$\underset{-}{L}^*=\min_{u \in U}\{L(u,v*)\}$

如果r1和r2分别表示P1和P2所体验到的后悔量，总的后悔度可以表示为：
$r_1 + r_2 = \overset{-}{L^*}-\underset{-}{L}^*$

只有满足$ \overset{-}{L^*}-\underset{-}{L}^*$时，两个玩家才都会满意。此时的状态也被称为鞍点。

**** 随机策略
博弈论中有一个著名的结论是在随机策略的空间中，对于零和博弈来说鞍点总是存在的，
但是需要使用到期望代价。（冯诺依曼证明）

根据如下观察可以找到随即策略的鞍点：
+ 通过仅仅考虑对手的确定性策略可以找到每个玩家的详细策略。
+ 如果每个玩家的策略是固定的，那么期望代价就是不确定概率的线性方程。

*** 非零和博弈
一对动作被定义为纳什均衡，如果：
$L_1(u^*,v^*)=\min_{u \in U}\{L_1(u,v^*)\}$
$L_2(u^*,v^*)=\min_{v \in V}\{L_2(u^*,v)\}$

很多纳什均衡通过可以使用Pareto最优来进行消除。
需要注意的是，可能并不存在纳什均衡，但是随机的纳什均衡总是存在的，
不过求解比较困难。

*** 详细的决策论
对于之前介绍的决策模型有着下面的问题是需要回答的：
代价值是如何决定的？
我们为何相信优化期望代价是正确的事？
如果主概率分布是不可知的？
最差情况分析是不是过于保守？
玩家知道对方的代价函数是否实际？

** 连续的决策模型
在本章中当前状态总是已知的。
唯一的不确定性在于预测未来状态。

*** 和自然的连续博弈的介绍
**** 模型定义
在每个阶段k中，自然的动作 $\theta_k$ 是从集合 $\Theta(x_k,u_k)$ 中选取的。

在概率模型中自然的选择是符合马尔可夫模型的，
也就是其概率分布取决与当前信息。

定义状态和动作的历史为：
$\overset{-}{x_k}=(x_1,x_2,...,x_k)$
$\overset{-}{u_k}=(x_1,x_2,...,x_k)$

根据马尔可夫假设可得：
$P(\theta_k|\overset{-}{x}_k,\overset{-}{u}_k)=P(\theta_k|x_k,u_k)$

一个阶段累计的代价函数L可表示为：
$L(\overset{-}{x_F},\overset{-}{u_K},\overset{-}{\Theta_k})=\sum_{k=1}^K{l(x_k,u_k,\Theta_k)}+l_F(x_F)$
可通过设定不同的代价值来达到目标。

**** 前向投影和后向投影
***** 前向投影
根据 $X_k$ 推 $X_{k+1}$

***** 后向投影
有时定义从当前点可以获得的可能的之前状态会是非常有用的。

**** 规划及执行
由于环境干扰的存在，不同的未来状态是可以获得的。
这需要使用结合使用反馈和将状态投影到动作的规划。

最终的代价取决于被访问的状态的序列，规划所采取的动作
以及自然所采用的应对动作。

用 $H(\pi,x_1)$ 表示当以 $x_1$ 为初始点时，
采用 $\pi$ 为方案时的状态-动作-自然的历史信息。

最差情况分析表示为：
$G_{\pi}(x_1)=\max_{(\overset{-}{x},\overset{-}{u},\overset{-}{\theta}) \in H(\pi,x_1)}
\{L(\overset{-}{x},\overset{-}{u},\overset{-}{\Theta})\}$

期望情况分析表示为：
$G_{\pi}(x_1)=E_{H(\pi,x_1)}
[L(\overset{-}{x},\overset{-}{u},\overset{-}{\Theta})]$

*** 计算反馈计划的算法
**** Value Iteration
基于动态规划的value Iteration算法可以通过扩展，
来很好地解决状态预测不确定的问题。
在当前的设定中，value iteration保持了绝大部分的效率，
并且很容易解决涉及成千上万的状态。

代价值的迭代存在下面两种情况：
非确定性的情况：
$G_k^*(x_k)=\min_{u_k \in U(x_k)}\{\max_{\theta_k}\{l(x_k,u_k,\theta_k)+G_{k+1}^*(x_{k+1})\}\}$

概率性的情况：
$G_k^*(x_k)=\min_{u_k \in U{x_k}}\{l(x_k,u_k)+\sum_{x_{k+1} \in X}G_{k+1}^*(x_{k+1})P(x_{k+1}|x_k,u_k)\}$

在不确定性的情况中，
为了使静态的cost-to-go方程获得收敛的效果，
不能存在负值的圈。

而在概率性的情况中，由于概率的存在，
还存在一种渐进收敛的情况。

上面两种情况所对应的最优方案如下所示：
$\pi^*(x)=\argmin_{u \in U(x)} \{\max_{\theta \in \Theta(x,u)}\{l(x,u,\theta)+G^*(f(x,u,\theta))\}\}$
$\pi^*(x)=\argmin_{u \in U(x)} \{E_{\theta}\{l(x,u,\theta)+G^*(f(x,u,\theta))\}\}$

**** Policy Iteration
Policy Iteration算法：
+ 选取一个任意方案 $pi$ ,其中 $u_T$被应用于
每个状态 $x \in X_G$，并且其它的动作都被任意选取。
+ 使用下式对每个 $x \in X$计算 $G_{\pi}$：
$G_{\pi}(x)=l(x,\pi(x))+\sum_{x' \in X}G_{\pi}(x')P(x'|x,\pi(x))$
+ 替换 $G^*$ 为计算的 $G_{\pi}$值，并计算一个更好的计划，$\pi'$ :
$\pi'(x)=\argmin_{u \in U(x)}\{l(x,u)+\sum_{x' \in X}G_{\pi}(x')P(x'|x,u)\}$
+ 比较 $\pi$ 和 $\pi'$，再继续迭代。

Policy iteration和value iteration相比（一个只考虑一个动作，另一个考虑所有动作），
它的收敛速度更快。

**** Graph Search Method
value iteration非常通用，但是，在很多情况下，
或者由于最优的cost-to-go已经知道了，或者终点还没到达，
大多数时间都被浪费在并不更新自身值的状态上。
Policy iteration虽然在一定程度上缓解了这个问题，
但是也局限于小的状态空间中。

一个后向搜索方法可以通过后向投影算法从 $X_G$开始不断的增长方案获得。

*** 无限水平问题
当需要迭代的阶段为无穷多次时，问题变的更加负责了，
这个问题也被称为无限水平问题。

为了使得问题可解，需要迫使累计性的代价成为有限的，
即使存在无限多的阶段。

其中代表性的有两种代价模型：
1.discounted cost model
对于任意的参数 $\alpha \in (0,1)$
$lim_{K->\infty}(\sum_{k=0}^K{\alpha^k})=\frac{1}{1-\alpha}$

其对应的代价函数为：
$L(\overset{-}{x},\overset{-}{u},\overset{-}{\theta})=lim_{K->\infty}(\sum_{k=0}^K{\alpha^kl(x_k,u_k,\theta_k)})<=lim_{K->\infty}(\sum_{k=0}^K{\alpha_kc})$

2.average cost-per-stage model
$L(\overset{-}{x},\overset{-}{u},\overset{-}{\theta})=lim_{K->\infty}(\frac{1}{K}\sum_{k=0}^{K-1}l(x_k,u_k,\theta_k))$
其接下来的推导也使用到了最大的边界值c。

针对这两种模型，均可使用value iteration和policy iteration方法进行求解。

*** 强化学习
可以为概率性的无限水平问题来求解最优方案。
基本的想法是将学习概率分布 $P(\theta|x,u)$ 的问题和计算最优方案的算法相结合。

最后使用Q-learning方法将“学习”的迭代式和value iteration(policy iteration)相结合。

*** 连续化的博弈理论
**** 博弈树
树型的表示通常被认为是一个博弈的扩展形式。

对于博弈树，存在三种信息模型：
+ 可替代的对手：选手轮流进行游戏，并且所有的选手均知道之前采取的动作。
+ 分阶段的模型：所有的选手都知道之前阶段所采取的动作，但是没有当前阶段的信息。
+ 开放式：每个选手并不知道之前的动作。

对于这三种不同的模型，分别可以采用minmax树分析，计算双方均满意的saddle point，
将树转化为单一阶段的游戏。

由于存在不同的动作序列到达相同的状态的情况，
可以通过声明相同的状态相等来将博弈树转化为博弈图的方法来简化运算。

**** 其他连续化博弈
***** Nash均衡点
在连续化的博弈树中，可能存在很多的Nash均衡点。
其计算和表示会变得非常有挑战。

***** 引入自然
非确定性情况中，可以根据最差情况后悔度的分析矩阵，来判断是否可能消除后悔值。

在概率性的情况中，可以根据概率来对不同的情况进行结合。
这种情况下的连续化博弈被称为Markov博弈。

***** 引入更多选手
在这种情况中，很多不同的信息模型均是可以应用的。
其状态转移方程可以由下式表示：
$x_{k+1}=f(x_k,u_k^1,u_k^2,...,u_k^n)$

*** 连续的状态空间
离散化

* Curvature Path Planning with High Resolution Graph for Unmanned Surface Vehicle
大多数针对USV的基于栅格地图的路径规划算法，
忽视了无人艇的最大拐角的限制，
仅仅考虑了平面二自由度。

本文提出了，构造一个不均匀的栅格地图，集合上的代价值，
之后扩展了USV的维度来反应USV的几何约束（将栅格地图扩展了一个角度的维度，并且限制了转角范围为+-5度）
最后结合USV的动力学约束来提出一个新的代价函数（考虑了代价地图的值，USV的当前转弯半径和启发式代价值）。

* Theta*: Any-Angle Path Planning for Smoother Trajectories in Continuous Environments
** Theta*和A*
由于其简单性和最优的保证，A*总是搜索算法的好的选择。
但是A*在图中所搜索出的最短路径，却和实际环境的最短路径并不相同。

A*搜索产生的长并且不实际的搜索路径的问题是被广泛知晓的，具体可见Game Programming Gems: A* Aesthetic Optimizations
[5] S. Rabin。
其所产生的路径很多都是曲折，拥有很多拐角的，
一种解决方案是应用后处理技术来优化路径，但是A*所生成的不同的图最优路径优化之后却可能相差很大。

Theta*沿着图的边传递信息，但其路径却不局限于图的边。
其和A*最大的区别是Theta*允许其父节点为任意节点。

** Theta*的算法

#+BEGIN_SRC python
  def ComputeCost(s, news):
      if LineOfSight(parent(s), news):
          if g(parent(s)) + c(parent(s), news) < g(news):
              parent(news) = parent(s)
              g(news) = g(parent(s)) + c(parent(s), news)
      else:
          if g(s) + c(s, news) < g(news):
              parent(news) = s
              g(news) = g(s) + c(s, news)
#+END_SRC

** 分析
Theta*并不总能找出连续环境中的最短路径，
但是它却能在大多数情况下找出最短路径。

* Angular rate-constrained path planning algorithm for unmanned surface vehicles
** 算法背景
本问对Theta*算法进行了改进，提出了ARC-Theta*算法，
其可以根据实际的航向角和转向性能来实时地生成路径。

因为传统的Theta*算法不能在有权重的栅格地图中工作，
Choi和Yu提出了一个改进的Theta*算法来避免Corner-node表示问题。

** Angular rate-constrained Theta* algorithm
该算法的一个关键点就是限制LOS的范围和转角速度来适应无人艇的转角性能。
转角速度定义如下：
$r=\frac{V}{R}$

其中r为转角速度，V为航行速度，R为转弯半径。

在ARC算法中通过对于转角过大的路径认为是不可行的，
从而优化路径。

*** 一些细节
由于海洋环境的动态性和不稳定的特性，
无人艇的转弯半径是很难预测的。
A maritime security committee in IMO
established a regulation called standards for ship maneuverability.
该文献限制了船的最小转弯半径为船长的5倍。

LOS方程考虑了太多的occupancy states，它的性能就会下降，
可以通过使用一个预先计算的occupancy table来进行改进。

*** 路径优化
通过使用这种算法，可以生成不规则的角度；
即使如此，该种方法生成的路径并不能满足终点处的到达航向角的需求。
因此在起点和终点处，使用了Dubin's Curve algorithm。

如果障碍物存在于以最大转弯率优化生成的路径上，
该问题可以通过减少转弯率并重新计算Dubin's curve来解决。

* Any-angle Path Planning on Non-uniform Costmaps
** 背景
非均匀的代价地图对于表示栅格地图的连通性非常有效，
但Basic Theta*对代价地图的使用都是基于均匀地图的（
其更新计算代价的过程中没有考虑所经过路径的代价）。
因此本文提出了两种代价函数来扩展Theta*方法，
使其能够对非均匀代价地图进行处理。

** 地图表示细节
栅格地图有两种不同的表示方法：
+ 基于中心节点的表示
+ 基于边缘节点的表示

在本文中采用的是中心节点的表示，因为：
+ 中心节点的表示被采用的更多，被研究的也最广泛
+ 边缘节点的表示会引起复杂和模糊的情况

** 非均匀代价地图中的Theta*
*** 首先，Theta*算法需要选择一个能够最小化路径代价的父节点
*** 接着，Theta*需要根据每个在line-of-sight上的栅格代价计算路径的代价
$edge(c_p,c_c)=\overset{-}{v}d(c_p,c_c)$
其中 $\overset{-}{v}$ 可以通过两种方式计算：
**** 几何平均
$\overset{-}{v}=\sum_{c_i \in L}\frac{1}{N}v(c_i)$

**** 加权平均
$\overset{-}{v}=\sum_{c_i \in L}\frac{w(c_i)}{\delta_x}v(c_i)$

一个边的代价特别是在较短路径长度的情况下会产生较大误差，
而加权平均方法虽然计算的是正确的代价值，但却会带来较大的计算量。

通常情况下使用加权平均是合适的，因为其引起的计算量的增加仅为百分之十。


* 无人艇的路径规划为什么要考虑海洋扰动
很多文献假设艇的路径规划不需要考虑海洋的扰动，但这确是不现实的。

* 海洋扰动下的无人艇控制模型有哪些
* 针对不同的模型有哪些规划方法，各有什么利弊
* 考虑海洋扰动后全局路径规划和局部路径规划有什么区别，采用哪种模型和方法好些
全局路径规划的准则？
