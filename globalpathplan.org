#+TITLE: 全局路径规划的总结
#+AUTHOR: Green 
#+DATE: \today

#+STARTUP: latexpreview
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+OPTIONS: TeX:t LaTeX:t skip:nil d:nil tasks:nil pri:nil title:t
#+LATEX_CLASS: xelatex-org-article 

** Planning algorithm
*** Discrete planning
**** Value Iteration
 根据最优方案的部分方案也为最优的观察，
 设计出一个得到最优结果的迭代式算法，称为数值迭代算法。
 可以求解变长规划问题，随机不确定性问题，
 不完美的状态测量问题以及其他一些复杂问题。

***** Optimal Fixed-Length Plans
****** 后向数值迭代
 $G^*_k(x_k)=\min_{u_k}\{l(x_k,u_k)+G^*_{k+1}(x_{k+1})\}$
 $G^*_k$ 的计算的时间复杂度为 $O(|X||U|)$ 。

****** 前向数值迭代
 在后向数值迭代中，$X_G$ 是固定的，而在前向数值迭代中pulling out my hair as 固定的为 $x_I$ 。

***** Optimal Plans of Unspecificed Length
 该模型并不包含特定的K，而是引入了一个特殊的动作 $u_T$ 。

 后向数值迭代公式：
 $G^*(x)=\min_u \{l(x,u) + G^*(f(x,u))\}$

 前向数值迭代公式：
 $u^* = \argmin_{u^{-1} \in U^{-1}(x)} \{C^*(f^{-1}(x,u^{-1}))+l(f^{-1}(x,u^{-1}),u')\}$

*** Basic Decision Theory
 一个叫做自然的特殊的状态决策器被使用作为一个通用的方法来模拟不确定性。
 自然在某种意义上来说是被虚构化的，因为它不是一个为了自己的利益做出智能理性决策的实体。

**** 基础概念
 最优解并不总是存在。

 定义一个部分排序 $<=$ ，在一系列动作上。
 一个动作 $u$ 被认为是主宰另一个动作 $u$ 如果他们并不相等，
 并且对于所有的 $i$ ，均有 $L_i(u)<=L_i(u')$ 。
 需要注意的是很多动作是不可比较的。

 一个动作被认为是Pareto optimal如果它不被任何其他的变量所主宰。

 并不存在随即策略比所有确定性策略都好。

**** 和自然的博弈
***** 对自然的建模
 和自然的博弈：$\theta\in\Theta$

 自然知道机器人的动作： $\theta\in\Theta(u)$

***** 非确定性和概率模型
 选取哪个模型取决于机器人所拥有的关于自然如何选取自己的反应动作的信息。
 + 非确定性：我完全不知道自然该如何动作。
 + 概率性：我已经观察过自然并且搜集到了有用信息。

 下面来对自然的建模和自然的博弈为例来说明。

 对于非确定性模型，在该种情况下的一个合理的方案是通过假设最差情况来做出决策。
 但是在很多应用中，最差条件分析过于保守。
 $u^*=\argmin_{u \in U}\{\max_{\theta\in\Theta}\{L(u,\Theta)\}\}$

 在概率模型中，采用的是期望条件分析。
 $u^*=\argmin_{u \in U}\{E_{\theta}[L(u,\theta)]\}$
 $E_{\theta}[L(u,\theta)]=\sum_{\theta\in\Theta}L(u,\theta)P(\theta)$

 在决策论中，关键想法是最小化后悔。
 这个是当做出坏的决策后希望反悔的一种感觉。
 可以使用下式描述：
 $T(u,\theta)=\max_{u' \in U}\{L(u,\theta)-L(u',\theta)\}$

**** 两个玩家的零和博弈
***** 博弈方程
 在本模型中，假设决策器有截然相反的利益。
 在博弈中，存在两个玩家，每个都以对方的损失为代价来获得利益。
 这在决策论中称为零和游戏。
 $L_1(u,v)=-L_2(u,v)$

***** 确定性策略
 对于P1来说，一个详细的策略u*可以定义为：
 $u*=\argmin_{u \in U}\{\max_{v\inV}\{L(u,v)\}\}$

 对于代价值，定义上界值和下界值如下：
 $\overset{-}{L^*}=\max_{v \in V}\{L(u^*,v)\}$
 $\underset{-}{L}^*=\min_{u \in U}\{L(u,v*)\}$

 如果r1和r2分别表示P1和P2所体验到的后悔量，总的后悔度可以表示为：
 $r_1 + r_2 = \overset{-}{L^*}-\underset{-}{L}^*$

 只有满足 $ \overset{-}{L^*}-\underset{-}{L}^*$ 时，两个玩家才都会满意。此时的状态也被称为鞍点。

***** 随机策略
 博弈论中有一个著名的结论是在随机策略的空间中，对于零和博弈来说鞍点总是存在的，
 但是需要使用到期望代价。（冯诺依曼证明）

 根据如下观察可以找到随即策略的鞍点：
 + 通过仅仅考虑对手的确定性策略可以找到每个玩家的详细策略。
 + 如果每个玩家的策略是固定的，那么期望代价就是不确定概率的线性方程。

**** 非零和博弈
 一对动作被定义为纳什均衡，如果：
 $L_1(u^*,v^*)=\min_{u \in U}\{L_1(u,v^*)\}$
 $L_2(u^*,v^*)=\min_{v \in V}\{L_2(u^*,v)\}$

 很多纳什均衡通过可以使用Pareto最优来进行消除。
 需要注意的是，可能并不存在纳什均衡，但是随机的纳什均衡总是存在的，
 不过求解比较困难。

**** 详细的决策论
 对于之前介绍的决策模型有着下面的问题是需要回答的：
 代价值是如何决定的？
 我们为何相信优化期望代价是正确的事？
 如果主概率分布是不可知的？
 最差情况分析是不是过于保守？
 玩家知道对方的代价函数是否实际？

*** 连续的决策模型
 在本章中当前状态总是已知的。
 唯一的不确定性在于预测未来状态。

**** 和自然的连续博弈的介绍
***** 模型定义
 在每个阶段k中，自然的动作 $\theta_k$ 是从集合 $\Theta(x_k,u_k)$ 中选取的。

 在概率模型中自然的选择是符合马尔可夫模型的，
 也就是其概率分布取决与当前信息。

 定义状态和动作的历史为：
 $\overset{-}{x_k}=(x_1,x_2,...,x_k)$
 $\overset{-}{u_k}=(x_1,x_2,...,x_k)$

 根据马尔可夫假设可得：
 $P(\theta_k|\overset{-}{x}_k,\overset{-}{u}_k)=P(\theta_k|x_k,u_k)$

 一个阶段累计的代价函数L可表示为：
 $L(\overset{-}{x_F},\overset{-}{u_K},\overset{-}{\Theta_k})=\sum_{k=1}^K{l(x_k,u_k,\Theta_k)}+l_F(x_F)$
 可通过设定不同的代价值来达到目标。

***** 前向投影和后向投影
****** 前向投影
 根据 $X_k$ 推 $X_{k+1}$

****** 后向投影
 有时定义从当前点可以获得的可能的之前状态会是非常有用的。

***** 规划及执行
 由于环境干扰的存在，不同的未来状态是可以获得的。
 这需要使用结合使用反馈和将状态投影到动作的规划。

 最终的代价取决于被访问的状态的序列，规划所采取的动作
 以及自然所采用的应对动作。

 用 $H(\pi,x_1)$ 表示当以 $x_1$ 为初始点时，
 采用 $\pi$ 为方案时的状态-动作-自然的历史信息。

 最差情况分析表示为：
 $G_{\pi}(x_1)=\max_{(\overset{-}{x},\overset{-}{u},\overset{-}{\theta}) \in H(\pi,x_1)}
 \{L(\overset{-}{x},\overset{-}{u},\overset{-}{\Theta})\}$

 期望情况分析表示为：
 $G_{\pi}(x_1)=E_{H(\pi,x_1)}
 [L(\overset{-}{x},\overset{-}{u},\overset{-}{\Theta})]$

**** 计算反馈计划的算法
***** Value Iteration
 基于动态规划的value Iteration算法可以通过扩展，
 来很好地解决状态预测不确定的问题。
 在当前的设定中，value iteration保持了绝大部分的效率，
 并且很容易解决涉及成千上万的状态。

 代价值的迭代存在下面两种情况：
 非确定性的情况：
 $G_k^*(x_k)=\min_{u_k \in U(x_k)}\{\max_{\theta_k}\{l(x_k,u_k,\theta_k)+G_{k+1}^*(x_{k+1})\}\}$

 概率性的情况：
 $G_k^*(x_k)=\min_{u_k \in U{x_k}}\{l(x_k,u_k)+\sum_{x_{k+1} \in X}G_{k+1}^*(x_{k+1})P(x_{k+1}|x_k,u_k)\}$

 在不确定性的情况中，
 为了使静态的cost-to-go方程获得收敛的效果，
 不能存在负值的圈。

 而在概率性的情况中，由于概率的存在，
 还存在一种渐进收敛的情况。

 上面两种情况所对应的最优方案如下所示：
 $\pi^*(x)=\argmin_{u \in U(x)} \{\max_{\theta \in \Theta(x,u)}\{l(x,u,\theta)+G^*(f(x,u,\theta))\}\}$
 $\pi^*(x)=\argmin_{u \in U(x)} \{E_{\theta}\{l(x,u,\theta)+G^*(f(x,u,\theta))\}\}$

***** Policy Iteration
 Policy Iteration算法：
 + 选取一个任意方案 $pi$ ,其中 $u_T$被应用于
 每个状态 $x \in X_G$，并且其它的动作都被任意选取。
 + 使用下式对每个 $x \in X$计算 $G_{\pi}$：
 $G_{\pi}(x)=l(x,\pi(x))+\sum_{x' \in X}G_{\pi}(x')P(x'|x,\pi(x))$
 + 替换 $G^*$ 为计算的 $G_{\pi}$值，并计算一个更好的计划，$\pi'$ :
 $\pi'(x)=\argmin_{u \in U(x)}\{l(x,u)+\sum_{x' \in X}G_{\pi}(x')P(x'|x,u)\}$
 + 比较 $\pi$ 和 $\pi'$，再继续迭代。

 Policy iteration和value iteration相比（一个只考虑一个动作，另一个考虑所有动作），
 它的收敛速度更快。

***** Graph Search Method
 value iteration非常通用，但是，在很多情况下，
 或者由于最优的cost-to-go已经知道了，或者终点还没到达，
 大多数时间都被浪费在并不更新自身值的状态上。
 Policy iteration虽然在一定程度上缓解了这个问题，
 但是也局限于小的状态空间中。

 一个后向搜索方法可以通过后向投影算法从 $X_G$开始不断的增长方案获得。

**** 无限水平问题
 当需要迭代的阶段为无穷多次时，问题变的更加负责了，
 这个问题也被称为无限水平问题。

 为了使得问题可解，需要迫使累计性的代价成为有限的，
 即使存在无限多的阶段。

 其中代表性的有两种代价模型：
 1.discounted cost model
 对于任意的参数 $\alpha \in (0,1)$
 $lim_{K->\infty}(\sum_{k=0}^K{\alpha^k})=\frac{1}{1-\alpha}$

 其对应的代价函数为：
 $L(\overset{-}{x},\overset{-}{u},\overset{-}{\theta})=lim_{K->\infty}(\sum_{k=0}^K{\alpha^kl(x_k,u_k,\theta_k)})<=lim_{K->\infty}(\sum_{k=0}^K{\alpha_kc})$

 2.average cost-per-stage model
 $L(\overset{-}{x},\overset{-}{u},\overset{-}{\theta})=lim_{K->\infty}(\frac{1}{K}\sum_{k=0}^{K-1}l(x_k,u_k,\theta_k))$
 其接下来的推导也使用到了最大的边界值c。

 针对这两种模型，均可使用value iteration和policy iteration方法进行求解。

**** 强化学习
 可以为概率性的无限水平问题来求解最优方案。
 基本的想法是将学习概率分布 $P(\theta|x,u)$ 的问题和计算最优方案的算法相结合。

 最后使用Q-learning方法将“学习”的迭代式和value iteration(policy iteration)相结合。

**** 连续化的博弈理论
***** 博弈树
 树型的表示通常被认为是一个博弈的扩展形式。

 对于博弈树，存在三种信息模型：
 + 可替代的对手：选手轮流进行游戏，并且所有的选手均知道之前采取的动作。
 + 分阶段的模型：所有的选手都知道之前阶段所采取的动作，但是没有当前阶段的信息。
 + 开放式：每个选手并不知道之前的动作。

 对于这三种不同的模型，分别可以采用minmax树分析，计算双方均满意的saddle point，
 将树转化为单一阶段的游戏。

 由于存在不同的动作序列到达相同的状态的情况，
 可以通过声明相同的状态相等来将博弈树转化为博弈图的方法来简化运算。

***** 其他连续化博弈
****** Nash均衡点
 在连续化的博弈树中，可能存在很多的Nash均衡点。
 其计算和表示会变得非常有挑战。

****** 引入自然
 非确定性情况中，可以根据最差情况后悔度的分析矩阵，来判断是否可能消除后悔值。

 在概率性的情况中，可以根据概率来对不同的情况进行结合。
 这种情况下的连续化博弈被称为Markov博弈。

****** 引入更多选手
 在这种情况中，很多不同的信息模型均是可以应用的。
 其状态转移方程可以由下式表示：
 $x_{k+1}=f(x_k,u_k^1,u_k^2,...,u_k^n)$

**** 连续的状态空间
 离散化
